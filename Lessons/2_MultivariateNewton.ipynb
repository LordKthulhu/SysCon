{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1976d24-8eac-4304-b0f4-d9c04d7a39e1",
   "metadata": {},
   "source": [
    "# Multivariate Newton method\n",
    "\n",
    "*[Written by: Maxime Pierre, 2023]* \\\n",
    "We now know from the basic idea behind Newton's method and how to implement it simply in `python`, in the case of function of a real variable. \\\n",
    "However, problems in continuum mechanics often involve systems of equations and/or vector/tensor variables. In this tutorial, we will walk through a simple 2D case: 2 unknowns, 2 equations. \\\n",
    "We will consider two planar curves, of following equations:\n",
    "$$ \n",
    "(E) : \\left\\{\n",
    "\\begin{array}{}\n",
    "f_1(x_1,x_2) = \\left(x_1-\\frac{1}{2}\\right)^2 + 2\\left(x_2+\\frac{1}{2}\\right)^2 - 1, \\\\\n",
    "f_2(x_1,x_2) = \\cos(x_2)x_1^2 + \\cos(x_1)x_2^2 - \\frac{1}{2}.\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "$f_1$ is the equation to an ellipse and $f_2$ to an ellipse modified with a cosinus factor. Solving this system of equations amounts to finding the intersection between these two ellipse-like curves in the $(x_1,x_2)$ plane. \\\n",
    "Let us first define the two functions $f_1$ and $f_2$ and plot the two curves. For this, we will use a `contour` plot and display only the contour corresponding to a value of the function equal to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bc008c-9791-413d-b5a0-7ad6c23a712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f1(x1, x2):\n",
    "    return ### Exercise: write f_1 ###\n",
    "\n",
    "def f2(x1, x2):\n",
    "    return ### Exercise: write f_2 ###\n",
    "\n",
    "fig = plt.figure()\n",
    "gc = fig.gca()\n",
    "x1 = np.linspace(-1.5, 1.5, 50)\n",
    "x2 = np.linspace(-1.5, 1.5, 50)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "gc.contour(x1, x2, f1(X1, X2), [0])\n",
    "gc.contour(x1, x2, f2(X1, X2), [0])\n",
    "gc.set_xlabel(r\"$x_1$\")\n",
    "gc.set_ylabel(r\"$x_2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3315df34-279e-4c56-ae78-02f15f1e7dac",
   "metadata": {},
   "source": [
    "As we can see, the two curves intersect in two locations. Let us see if we can find one of these intersections using Newton's method. Rather than scalar values, we will now deal with a vector unknown:\n",
    "$$ X = \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "Our residue can also be seen as a vector:\n",
    "$$\n",
    "F(X) = \\begin{bmatrix}\n",
    "f_1(X) \\\\\n",
    "f_2(X)\n",
    "\\end{bmatrix}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd84320-f3a6-4bda-a520-68135d7a2110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(X): # Defining the residu and unknown as vectors\n",
    "    x1, x2 = X\n",
    "    return ### Exercise: return the right numpy vector ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a44dd0b-badc-4ffd-ac09-9aefdc5014ea",
   "metadata": {},
   "source": [
    "Now, we have to have an equivalent of the derivative of $F$. In function of two variables, the differential of the function is defined as such:\n",
    "$$\n",
    "g : (x_1, x_2) \\rightarrow g(x_1, x_2),\n",
    "$$\n",
    "$$\n",
    "\\mathrm{d}g = \\frac{\\partial g}{\\partial x_1}\\mathrm{d}x_1 + \\frac{\\partial g}{\\partial x_2}\\mathrm{d}x_2,\n",
    "$$\n",
    "which can be seen as the following dot product:\n",
    "$$\n",
    "\\mathrm{d}g =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial g}{\\partial x_1} & \\frac{\\partial g}{\\partial x_2}\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "\\mathrm{d}x_1 \\\\\n",
    "\\mathrm{d}x_2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "For our case, we have a system consisting of two equations. We could abusively write:\n",
    "$$\n",
    "\\mathrm{d}F = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} \\\\\n",
    "\\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2}\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "\\mathrm{d}x_1 \\\\\n",
    "\\mathrm{d}x_2\n",
    "\\end{bmatrix}=\n",
    "J\\cdot \\mathrm{d}X,\n",
    "$$\n",
    "where $J$ is called the jacobian matrix of the system, playing the same role as the derivative in the 1D case. \\\n",
    "Note that generalization to $n$ dimensions is quite straightforward:\n",
    "$$\n",
    "J = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_n}{\\partial x_1} & \\cdots &\\frac{\\partial f_n}{\\partial x_n}\n",
    "\\end{bmatrix}=\n",
    "\\left( \\frac{\\partial f_i}{\\partial x_j} \\right)_{(i,j)}.\n",
    "$$\n",
    "Let us implement the jacobian matrix of our system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c30fa44-936f-4ef1-bd0b-a8835593515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(X):\n",
    "    x1, x2 = X\n",
    "\n",
    "    ### Exercise: calculate the partial derivatives\n",
    "    df1_dx1 = #\n",
    "    df1_dx2 = #\n",
    "    df2_dx1 = #\n",
    "    df2_dx2 = #\n",
    "    ###\n",
    "    return np.array([[df1_dx1, df1_dx2], [df2_dx1, df2_dx2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36b16d8-8afb-417d-8549-102115805bb3",
   "metadata": {},
   "source": [
    "Continuing our parallels with the 1D method, the iteration will be the following:\n",
    "$$\n",
    "X_{n+1} = X_n - J^{-1}(X_n)\\cdot F(X_n).\n",
    "$$\n",
    "This of course requires $J(X_n)$ to be non-singular, just as the 1D method required the derivative to be non-zero. \\\n",
    "Evaluation of the convergence will be done through the norm of the residu, with the following convergence criterion for a precision $\\varepsilon > 0$:\n",
    "$$\n",
    "\\| F(X_n)\\| \\leq \\varepsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702123d0-461e-42c8-83e2-cbb253cc4e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_method(residu, jacobian, initial_guess, tolerance, max_iteration=50):\n",
    "    X_n = initial_guess\n",
    "    X = [initial_guess]\n",
    "    iteration = 0\n",
    "    \n",
    "    while np.linalg.norm(residu(X_n)) > tolerance and iteration < max_iteration:\n",
    "        \n",
    "        ### Exercise: update X_n ###\n",
    "        \n",
    "        X.append(X_n.copy())\n",
    "        iteration += 1\n",
    "        print(\"Iteration {}: X_{} = {}, residual norm = {}\".format(iteration, iteration, X_n, np.linalg.norm(residu(X_n))))\n",
    "        \n",
    "    if np.linalg.norm(residu(X_n)) > tolerance: # Not converged\n",
    "        print(\"Did not converge after {} iterations.\".format(iteration))\n",
    "    else: # Converged\n",
    "        print(\"Converged in {} iterations.\".format(iteration))\n",
    "        \n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d758ec0-e7f8-4130-8d56-50456f99a31a",
   "metadata": {},
   "source": [
    "Let us test the algorithm with the following initial guess:\n",
    "$$\n",
    "X_0 = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "1\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "and a tolerance of $10^{-12}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f691279-0296-4e39-b717-2cd908eb33ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_0 = [1,1]\n",
    "eps = 1e-12\n",
    "\n",
    "result = newton_method(F, J, X_0, eps)\n",
    "\n",
    "gc.scatter((result.T)[0], (result.T)[1], c='r')\n",
    "for i in range(len((result.T)[0])):\n",
    "    gc.annotate( r\"$X_{}$\".format(i), (result[i,0], result[i,1]), c='r' )\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277865cb-0a85-42b7-bb44-52c8b950611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_0 = [0.5,0.5]\n",
    "\n",
    "result = newton_method(F, J, X_0, eps)\n",
    "\n",
    "gc.scatter((result.T)[0], (result.T)[1], c='b')\n",
    "for i in range(len((result.T)[0])):\n",
    "    gc.annotate( r\"$X_{}$\".format(i), (result[i,0], result[i,1]), c='b' )\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fa05bb-9370-4945-8169-68a996609363",
   "metadata": {},
   "source": [
    "## Bonus: Newton's method with numerical approximation of the jacobian\n",
    "\n",
    "Calculating the Jacobian matrix of the system can be pretty tedious at times (especially later when dealing with tensorial equations for instance). An alternative way consists in using an numerical approximate of the jacobian obtained by a small perturbation $\\eta$. The partial derivatives, components of the jacobian matrix can be approximated as: \\\n",
    "$$\n",
    "\\frac{\\partial f_i}{\\partial x_j}\\left( (\\tilde{x}_k)_{1\\leq k \\leq n}\\right) \\simeq \\frac{ f_i( (\\tilde{x_k} + \\delta_{jk}\\eta)_{1\\leq k\\leq n}) - f_i( (\\tilde{x_k} - \\delta_{jk}\\eta)_{1\\leq k\\leq n})}{ 2\\eta },\n",
    "$$\n",
    "where $\\delta_{jk}$ is Kronecker's delta. Let us implement it for our previous case and test it with $x_0 = [1,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce82215b-c281-4341-aefd-7cdb8fd5ae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_J(X, residu, perturbation):\n",
    "    ### Exercise: write a function which returns the numerical jacobian\n",
    "\n",
    "X_0 = [1,1]\n",
    "eta = 1e-9 # Perturbation magnitude for the numerical jacobian\n",
    "\n",
    "result = newton_method(F, lambda X : numerical_J(X, F, eta), X_0, eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1982907-70b1-44b8-9559-608947d396ee",
   "metadata": {},
   "source": [
    "If we look closely at the residual value, we can see that we lost some precision compared to the exact jacobian. However, it did not affect our convergence overall."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
